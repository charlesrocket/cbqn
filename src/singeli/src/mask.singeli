local def maskInit1{w} = {
  apply{merge, each{{x} => {
    merge{(w/8-1)**255, (1<<x)-1, (w/8)**0}
  }, iota{8}}}
}
mask256_1:*u8 = maskInit1{256}; def maskOfBit{T,n if width{T}==256} = load{*[32]u8  ~~  (mask256_1 + (n>>3)^31 + 64*(n&7))}
mask128_1:*u8 = maskInit1{128}; def maskOfBit{T,n if width{T}==128} = load{*[16]u8  ~~  (mask128_1 + (n>>3)^15 + 32*(n&7))}

mask256:*i64 = merge{4 ** -1,  4 ** 0}
local def maskOfImpl{T, n, w} = load{*ty_u{T} ~~ (*u8~~mask256 + 32 - n*(elwidth{T}/8))}

# get homogeneous mask of first n items; 0 ≤ n ≤ vcount{T}
def maskOf{T,n if w256{T}} = maskOfImpl{T, n, 256}
def maskOf{T,n if w128{T}} = maskOfImpl{T, n, 128}
def maskOf{T,n if  w64{T}} = maskOfImpl{T, n,  64}

def anyne{x:T, y:T, M if M{0}==0 and isvec{T}} = ~all_hom{x==y}
def anyne{x:T, y:T, M if M{0}==1 and isvec{T}} =  any_hom{M{x!=y}}
def anyne{x:T, y:T, M if M{0}==0 and anyInt{x}} = x!=y
def anyne{x:T, y:T, M if M{0}==1 and anyInt{x}} = M{x^y} != 0
def anyneBit{x:T, y:T, M} = ~M{x^y, 'all bits zeroes'}

def anynePositive{x:T, y:T, M if M{0}==0} = anyne{x, y, M}
def anynePositive{x:T, y:T, M if M{0}==1 and isvec{T}} = {
  def {n,m} = hom_to_int_ext{x==y}
  def E = tern{type{m}==u64, u64, u32}
  (promote{E,~m} << (width{E}-M{'count'}*n)) != 0
}

def maskNone{x} = x
def maskNone{x, 'all bits zeroes'} = andAllZero{x, x}
def maskAfter{n} = {
  def mask{x:X, 'all bits zeroes'} = andAllZero{x, X~~maskOfBit{X,n}}
  def mask{X, 'to sign bits'} = maskOf{X,n}
  def mask{X, 'to homogeneous bits'} = maskOf{X,n}
  def mask{'count'} = n
  def mask{{x}} = tup{mask{x}}
  def mask{x:X if isvec{X}} = x & (X~~maskOf{X,n})
  def mask{x:X if anyInt{x}} = x & ((1<<n) - 1)
  def mask{0} = 1
}



def loadLowBatch{V=[_]T, ptr:*T, w, n} = loadLow{*V ~~ (ptr + n*(w/width{T})), w}

# store the i-th batch of k elements to ptr, narrowing elements if needed; masked by M
def storeBatch{ptr:*E0, i, x:[k]E1, M} = {
  def rpos = ptr + i*k
  def TF = re_el{E0, [k]E1}
  xu:= narrow{E0, x}
  
  if (M{0}) homMaskStoreF{*TF~~rpos, M{TF, 'to homogeneous bits'}, undefPromote{TF, xu}}
  else storeLow{rpos, k*width{E0}, xu}
}

# (sign/zero)-extend the i-th batch of k elements at ptr to [k]E1
def loadBatch{ptr:*E0, i, [k]E1} = {
  def rpos = ptr + i*k
  def TF = re_el{E0, [k]E1}
  widen{[k]E1, loadLow{*TF~~rpos, k*width{E0}}}
}

def loadBatch {ptr:*E, {...ns}, T    } = each{loadBatch {ptr, ., T   }, ns}
def storeBatch{ptr:*E, {...ns}, xs, M} = each{storeBatch{ptr, ., ., M}, ns, xs}

# TODO also similar any_hom & use those more
def all_hom{(maskNone), ...xs} = all_hom{...xs}
def all_hom{M, x:T if kgen{M}} = ~any_hom{M{~x}} # TODO better

# "harmless" pointer cast that'll only cast void*
def hCast{T,p} = assert{0, 'expected pointer with element',T,'or void but got ',p}
def hCast{T,p:*T} = p
def hCast{T,p:(*void)} = *T~~p

def mlExec{i, iter, vars0, bulk, M} = {
  def vproc{p:*E} = p
  def vproc{'m'} = tptr{{_}=>M, '!'}
  
  def vproc{{T,p:*E}} = tptr{{i} => loadBatch{p, i, T}, {i,x} => storeBatch{p, i, x, M}}
  def vproc{{'b',  p:*E}} = tptr{{i} => b_getBatch{bulk, hCast{u64,p}, i}, '!'}
  def vproc{{'b',T,p:*E}} = tptr{{i} => loadBatchBit{T, hCast{u64,p}, i}, '!'}
  def vproc{{'g',  p:*E}} = tptr{{i} => ({x} => storeBatch{p, i, x, M}), '!'}
  def vproc{{'g',T,p:*E}} = tptr{{i} => {
    def dv{} = loadBatch{p, i, T}
    def dv{x} = storeBatch{p, i, x, M}
  }, '!'}
  
  iter{i, each{vproc, vars0}}
}

# i0 - initial batch index; not used as begin because it's in a different scale compared to end
def maskedLoop{bulk, i0}{vars,begin==0,end,iter} = {
  l:u64 = end
  
  m:u64 = l / bulk
  @for (i from i0 to m) mlExec{i, iter, vars, bulk, maskNone}
  
  left:= l & (bulk-1)
  if (left!=0) mlExec{m, iter, vars, bulk, maskAfter{left}}
}
def maskedLoop{bulk} = maskedLoop{bulk,0}


def maskedLoopPositive{bulk}{vars,begin==0,end:L,iter} = {
  assert{end > 0}
  i:L = 0
  while(i < (end-1)/bulk) {
    mlExec{i, iter, vars, bulk, maskNone}
    ++i
  }
  mlExec{i, iter, vars, bulk, maskAfter{end - i*bulk}}
}



# masked unrolled loop
#  bulk: vector count
#  unr: unroll amount
#  fromunr (optional): {}=>{transition from unrolled to non-unrolled}
#  loop args:
#    begin must be 0
#    end is scalar element count
#    index given is a tuple of batch indexes to process
def muLoop{bulk, unr, fromunr}{vars,begin==0,end,iter} = {
  l:u64 = end
  
  m:u64 = l / bulk
  if (unr==1) {
    @for (i from 0 to m) mlExec{tup{i}, iter, vars, bulk, maskNone}
    
    left:= l & (bulk-1)
    if (left!=0) mlExec{tup{m}, iter, vars, bulk, maskAfter{left}}
  } else {
    if (m > 0) {
      i:u64 = 0
      if (unr <= m) {
        while ((i+unr) <= m) {
          def is = each{{j}=>i+j, iota{unr}}
          mlExec{each{{j}=>i+j, iota{unr}}, iter, vars, bulk, maskNone}
          i+= unr
        }
        fromunr{}
      }
      if (unr==2) {
        if (i!=m) mlExec{tup{i}, iter, vars, bulk, maskNone}
      } else {
        @for(j from i to m) mlExec{tup{j}, iter, vars, bulk, maskNone}
      }
    }
    
    left:= l & (bulk-1)
    if (left!=0) mlExec{tup{m}, iter, vars, bulk, maskAfter{left}}
  }
}
def muLoop{bulk, unr} = muLoop{bulk, unr, {}=>0}
