include './base'
include './vecfold'

if_inline (hasarch{'SSE2'}) {
  fn sum_vec{T}(v:T) = vfold{+, fold{+, mzip128{v, T**0}}}
  def fold_addw{v:T=[_]E if E<=u32} = sum_vec{T}(v)
}

def inc{ptr, ind, v} = store{ptr, ind, v + load{ptr, ind}}
def inc{ptr, ind} = inc{ptr, ind, 1}

def block_loop{V=[vec]T, n, iter} = {
  def block = (2048*8) / width{V} # Target vectors per block
  def b_max = block + block/4     # Last block max length
  assert{b_max < 1<<width{T}}     # Don't overflow count in vector section
  i:u64 = 0
  while (i < n) {
    # Number of elements to handle in this iteration
    r:u64 = n - i; if (r > vec*b_max) r = vec*block
    iter{r}
    i += r
  }
}

# Write counts /⁼x to tab and return ⌈´x
fn count{T}(tab:*usz, xp:*void, n:u64, min_allowed:T) : T = {
  def vbits = arch_defvw
  def vec = vbits/width{T}
  def uT = ty_u{T}
  def V = [vec]T
  x := *T~~xp
  mx:T = min_allowed  # Maximum of x
  block_loop{V, n, {r} => { # Handle r elements
    b := r / vec  # Vector case does b full vectors if it runs
    rv:= b * vec
    r0:u64 = 0    # Elements actually handled by vector case

    # Find range to check for suitability; return a negative if found
    # Also record number of differences dc
    # (double-counts at index vec but it doesn't need to be exact)
    xv := *V~~x
    jv := load{xv}; mv := jv; dc := -(jv != load{*V~~(x+1)})
    @for (xv, xp in *V~~(x-1) over _ from 1 to b) {
      jv = min{jv, xv}; mv = max{mv, xv}
      dc -= xp != xv
    }
    @for (x over _ from rv to r) { if (x<min_allowed) return{x}; if (x>mx) mx=x }
    jt := vfold{min, jv}
    mt := vfold{max, mv}
    if (jt < min_allowed) return{jt}
    if (mt > mx) mx = mt

    # Fast cases
    dt := promote{u64, fold_addw{dc}}
    nc := uT~~(mt - jt)  # Number of counts to perform: last is implicit
    if (dt < b * (vec/2) and dt*8 < b * promote{u64,nc}) {
      r0 = count_with_runs{x, tab, r}
    } else if (nc <= 24*vbits/128) {
      r0 = rv
      count_by_sum{T, V, [vec]uT, xv, b, tab, r0,
        promote{u64, uT~~jt}, # Starting count
        promote{u64, nc}      # Number of iterations
      }
    }

    # Scalar fallback and cleanup
    @for (x over _ from r0 to r) inc{tab, x}
    x += r
  }}
  mx
}

# Sum comparisons against each value (except one) in the range
def count_by_sum{T, V, U, xv, b, tab, r0, j0, m} = {
  total := trunc{usz, r0}    # To compute last count
  def count_each{js, num} = {
    j := @collect (k to num) trunc{T, js+k}
    c := copy{length{j}, U**0}
    e := each{{j}=>V**j, j}
    @for (xv over b) each{{c,e} => c -= xv == e, c, e}
    def add_sum{c, j} = {
      s := promote{usz, fold_addw{c}}
      total -= s; inc{tab, j, s}
    }
    each{add_sum, c, j}
  }
  m4 := m / 4
  @for (j4 to m4) count_each{j0 + 4*j4, 4}
  @for (j from 4*m4 to m) count_each{j0 + j, 1}
  inc{tab, trunc{T, j0 + m}, trunc{usz,total}}
}

# Count adjacent equal elements at once, breaking at w-element groups
# May read up to index n from x, hitting one element that's not counted
def count_with_runs{x, tab, n} = {
  def w = width{ux}
  m0:ux = 1 << (w-1) # Last element in each chunk ends a run
  bw := n / w
  @for (i to bw) {
    xo := x + i*w
    m := m0; mark_run_ends{xo, m}
    inc_marked_runs{xo, tab, m, m0}
  }
  bw * w  # Number of elements handled
}
# Switch to the normal scalar count if there aren't enough runs
def count_adapt_runs{x0, tab, n} = {
  def w = width{ux}
  m0:ux = 1 << (w-1)
  x := x0; r := n
  while (r > 0) {
    def skip_runs = makelabel{}
    b:usz = w
    if (rare{b > r}) { b = r; goto{skip_runs} }
    m := m0; mark_run_ends{x, m}
    if (popc{m} < w/2) {
      inc_marked_runs{x, tab, m, m0}
    } else {
      setlabel{skip_runs}
      @for (x over b) inc{tab, x}
    }
    x += b; r -= b
  }
}
def mark_run_ends{x:*T, m:(ux)} = {
  def vec = arch_defvw/width{T}
  def V = [vec]T
  @unroll (j to width{ux} / vec) {
    def jv = j*vec
    def lv{k} = load{*V~~(x + k)}
    m |= promote{ux, homMask{lv{jv} != lv{jv+1}}} << jv
  }
}
def inc_marked_runs{x, tab:*T, m, m0} = {
  def w = width{ux}
  # Iterate over runs marked in m
  jp:T = - T~~1
  while (m > m0) @unroll (2) {
    j := trunc{T, ctz{m}}
    inc{tab, load{x, j}, cast_i{T, j - jp}}
    jp = j; m &= m-1
  }
  # One step if popc{m} was odd, reducing branch mispredictions above
  inc{tab, load{x, w-1}, ((w-1) - jp) & -trunc{T, m>>(w-1)}}
}

# No count_by_sum: build each run mask then decide whether to use it
fn count_i32_i32(tab:*i32, x:*i32, n:usz) : void = count_adapt_runs{x, tab, n}

# For i←/⁼x, store r←128|i, and i-r sparsely: x is ∧(/r)∾oc/ov
# ov is sorted but may not be unique, and oc contains multiples of 128
# Return the shared length of ov and oc
fn count_sorted{T}(r:*u8, ov:*usz, oc:*usz, x:*T, n:usz) : usz = {
  def V = [arch_defvw/width{T}]T
  def block = 128
  i:usz = 0
  on:usz = 0
  def overflow{xu,c} = { store{ov, on, xu}; store{oc, on, c}; ++on }
  while (i < n) {
    rem := n - i
    xo := x + i
    xi := load{xo}
    def overflow{c} = overflow{cast_i{usz,xi}, c}
    xe := xo-1; def bxi{j} = xi == load{xe, j}
    if (block <= rem and bxi{block}) {
      # Gallop to find last block ending in xi
      d:usz = block
      d2 := undefined{usz}
      while ((d2=d+d) <= rem and bxi{d2}) d = d2
      l := (rem &~ (block-1)) - d; if (l > d) l = d
      # Target is in [d,d+l); shrink l
      while (l > block) {
        h := (l/2) &~ (block-1)
        m := d + h
        if (bxi{m}) d = m
        l -= h
      }
      overflow{d}
      rem -= d; if (rem == 0) return{on}
      i += d; xo += d; xi = load{xo}
    }
    # Count the next block normally
    if (rem > block) rem = block
    count_adapt_runs{xo, r, rem}
    rxi := load{r, xi}
    if (rxi >= block) {
      store{r, xi, rxi - block}
      overflow{block}
    }
    i += rem
  }
  on
}

export{'simd_count_i8',  count{i8}}
export{'simd_count_i16', count{i16}}
export{'simd_count_i32_i32', count_i32_i32}
export{'si_count_sorted_i8',  count_sorted{i8}}
export{'si_count_sorted_i16', count_sorted{i16}}
export{'si_count_sorted_i32', count_sorted{i32}}
